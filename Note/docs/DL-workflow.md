# Workflow of training a deep learning model
### Table of contents
* [Prepare dataset](#prepare-dataset)
* [Architechture](#architecture)   
* [Set up environment](#set-up-environment)
* [Training](#training)    
* [Testing](#testing)   
* [Evaluation](#evaluation)
* [Common errors](#commen-errors) 

### Prepare dataset
### Architecture
â˜‘ï¸ CNN    
â˜‘ï¸ GAN    
â˜‘ï¸ Transformer        
â˜‘ï¸ SSL    
â˜‘ï¸ USL        
### Set up environment
â˜‘ï¸ CPU or GPU    
â˜‘ï¸ Epochs    
â˜‘ï¸ Learning rate    
â˜‘ï¸ Batch size    
â˜‘ï¸ Optimizer     
â˜‘ï¸ Scheduler         
â˜‘ï¸ Log save for loss metric curves.    

### Training
â˜‘ï¸ Strategy 1: Two-step pipeline. For instance, coarse segmentation (ROI detection) --> fine segmentation.    
â˜‘ï¸ Strategy 2: Compound loss functions. Main loss and auxiliary loss with weights. For instance, Dice+CE, Dice+Focal.    
â˜‘ï¸ Strategy 3: Think if a gradient is needed in some parts.    
â˜‘ï¸ Strategy 4: Training time augmentation (TTA).

### Testing
â˜‘ï¸ test-time-training (TTT) if necessary
### Evaluation
â˜‘ï¸ Metrics  
### Common errors
â‰ï¸ **RuntimeError**: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.        
ğŸ’  Solution1: `retain_graph=True`    
ğŸ’  Solution2: check `torch.no_grad()` and `var.detach()`    
â‰ï¸ **AttributeError**: 'float' object has no attribute 'backward'    
ğŸ’  Solution: `object.ToTensor()`    
â‰ï¸ RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1024, 27648]], which is output 0 of AsStridedBackward0, is at version 3; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).    
ğŸ’  Solution: `output_copy = output.clone()      # avoid in-place opration`    
â‰ï¸ **IndexError**: Dimension out of range (expected to be in range of [-2, 1], but got 2)    
ğŸ’  Solution: check data shape    
â‰ï¸ AttributeError: 'tuple' object has no attribute 'shape'    
ğŸ’  Solution: check data shape convert to `a_array = np.array(a); print(a_array.shape)  # (3,)`   
â‰ï¸ RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn in `total_loss.backward()`    
ğŸ’  Solution: check the loss definition, it should have a gradient.    
â‰ï¸ **IndentationError**: unindent does not match any outer indentation level
ğŸ’  Solution: check indent    

â‰ï¸ **TypeError**: `logits = torch.squeeze(logits,dim=1)` squeeze() received an invalid combination of arguments - got (tuple, dim=int), but expected one of:
 * (Tensor input)
 * (Tensor input, int dim)
      didn't match because some of the arguments have invalid types: (!tuple of (Tensor, Tensor)!, dim=int)
 * (Tensor input, tuple of ints dim)
      didn't match because some of the arguments have invalid types: (!tuple of (Tensor, Tensor)!, !dim=int!)
 * (Tensor input, name dim)
      didn't match because some of the arguments have invalid types: (!tuple of (Tensor, Tensor)!, !dim=int!)

ğŸ’  Solution: check datatype and dimension    

