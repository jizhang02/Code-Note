{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical image segmentation with deep learning\n",
    "\n",
    "Welcome to our hands-on session on medical image segmentation with deep learning! During this session you will study in depth and apply the very popular deep learning U-Net architecture to perform the segmentation of 2D ultrasound images. This is a 30-years old problem which has taken an important step forward thanks to deep learning techniques. This session is based on a study we (Creatis lab.) recently published in IEEE transactions on Medical Imaging (DOI: 10.1109/TMI.2019.2900516). To provide answers to the problem of echocardiographic image segmentation by deep learning methods, we have set up a substantial open access dataset, named CAMUS, composed of 500 patients with two types of acquisitions: two-chamber and four-chamber views. During this hands-on session, you will use only a fraction of this dataset (restricted to four-chamber view acquisitions from 200 patients) to make the different learning phases feasible within the framework of this session. However, if you want to go further on this topic after this spring school, don't hesitate to download the full dataset and play with it! You will find all the necessary information at the following web address: http://camus.creatis.insa-lyon.fr/challenge/\n",
    "\n",
    "This hands-on session will:\n",
    "- allow you to study how to code a deep learning segmentation method with the Keras library (tensorflow backend)\n",
    "- give you hindsights on good practices when programming such deep learning architectures\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "- We recommend that you write your answers directly on this notebook. You will be able to retrieve the whole code for your personal use by simply saving this notebook (*.ipynb file) on your local session and then send it to your email account.\n",
    "\n",
    "**You will learn to:**\n",
    "- Use (program) the fundamental steps to design the architecture of a deep learning algorithm from the Keras library, including:\n",
    "    - Preprocessing the input data  \n",
    "    - Defining a deep learning model\n",
    "    - Setting up callback functions to better investigate the training phase \n",
    "    - tuning different key parameters and study their influence\n",
    "- Explore more advanced concepts to begin to demystify the black box effect of deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning U-Net architecture\n",
    "\n",
    "U-Net is one of the most famous deep learning architecture used for image segmentation in medical imaging. It has been created by by Olaf Ronneberger, Philipp Fischer and Thomas Brox in 2015 at the paper \"UNet: Convolutional Networks for Biomedical Image Segmentation\" (DOI: 10.1007/978-3-319-24574-4_28). The original architecture proposed in 2015 is given below.\n",
    "\n",
    "<br><img src=\"notebook_illustrations/u-net-architecture.png\" style=\"width:700px;height:466px;\"><br>\n",
    "\n",
    "The network consists of a contracting path and an expansive path, which gives it the u-shaped architecture. The contracting path is a typical convolutional network that consists of repeated application of convolutions, each followed by a rectified linear unit (ReLU) and a max pooling operation. During the contraction, the spatial information is reduced while feature information is increased. The expansive pathway combines the feature and spatial information through a sequence of up-convolutions and concatenations with high-resolution features from the contracting path. For more information, don't hesitate to visit the official website at https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\n",
    "\n",
    "<br>\n",
    "\n",
    "In this hands-on session, we will use this U-Net architecture to segment 2D echocardiography images. In particular, we will focus on the segmentation of three adjacent cardiac structures: the left ventricle, the myocardium and the right ventricle. An illustration of the segmentation (image on the right) of a typical case (image on the left) is given below. As you can see, the segmentation of these ultrasound images is particularly difficult due to many sources of artifacts, the recognition of the structures to segment, and the subjective delineation of the contours (e.g. at the lower part of the myocardial segmentation mask).  \n",
    "\n",
    "<br><img src=\"notebook_illustrations/us_seg_illustration.png\" style=\"width:735px;height:341px;\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#packages\">1&nbsp;&nbsp;Packages</a></div><div class=\"lev1 toc-item\"><a href=\"#dataset\">2&nbsp;&nbsp;Dataset (estimated time: 10 minutes)</a></div><div class=\"lev1 toc-item\"><a href=\"#architecture_unet\">3&nbsp;&nbsp;Definition of the architecture of a U-Net model (estimated time: 40 minutes)</a></div><div class=\"lev1 toc-item\"><a href=\"#training_phase\">4&nbsp;&nbsp;Training phase (estimated time: 40 minutes)</a></div><div class=\"lev1 toc-item\"><a href=\"#training_optimization\">5&nbsp;&nbsp;Training optimization (estimated time: 30 minutes)</a></div><div div class=\"lev1 toc-item\"><a href=\"#unet_optimization\">6&nbsp;&nbsp;U-Net architecture optimization (estimated time: 30 minutes)</a></div><div div class=\"lev1 toc-item\"><a href=\"#to_go_further\">7&nbsp;&nbsp;To go further (estimated time: remaining time)</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='packages'></a>\n",
    "## 1 - Packages\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this session. \n",
    "- [keras](https://keras.io/) is a high-level neural networks API, written in Python and capable of running on top of TensorFlow. It provides high-level built-in functions which simplify the design of deep learning architectures (https://keras.io/why-use-keras/).\n",
    "- [tensorflow](https://keras.io/) is a Python library for fast numerical computing created and released by Google that can be used to create deep learning models directly or by using wrapper libraries that simplify the process built on top of TensorFlow such as keras.\n",
    "\n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [medpy](http://loli.github.io/medpy/) is a library dedicated to medical imaging in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import *\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "!pip install medpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "In order to make the hands-on session more fluent, we have coded our own python modules. Some of these modules contain functions that call others keras functions. So, for participants who are not comfortable with python programming, we invite you to focus on running the cells of the notebook and analyze the generated results. For the others, we invite you to regularly take a look at the functions present in our modules (loadable from the file explorer) in order to analyze how the different functionalities we developed have been developed. \n",
    "\n",
    "Let's run the cell below to import our own modules that you will need during this session. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sources.model import unet\n",
    "from sources.dataset import load_camus\n",
    "from sources.metrics_and_losses import multiclass_dice, classes_dice_loss\n",
    "from sources.evaluation import get_geometrical_results, display_mean_values_of_metrics, display_histogram_of_metrics\n",
    "from sources.utils import LossHistory, display_dataset_samples\n",
    "from sources.utils import display_loss_metric_curves, display_dataset_samples_with_estimation \n",
    "from sources.utils import display_model_weights, display_model_feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>\n",
    "## 2 - Dataset\n",
    "    => estimated time: 10 minutes\n",
    "\n",
    "We'll first mount the dataset we will use and also a few pretrained models. In order to do that, on the right tab \"Add datasets\", enter the following :\n",
    "- Data : \"ge-insa-lyon/datasets/camus_dataset/2\"  \n",
    "- Mount directory : \"camus_dataset\" (default) -> Attach dataset\n",
    "- Data : \"ge-insa-lyon/datasets/camus_pretrained_models/1\"\n",
    "- Mount directory : \"camus_pretrained_models\" (default) -> Attach dataset\n",
    "\n",
    "\n",
    "As presented in the introduction of this hands-on session, you will use a subset of the CAMUS dataset restricted to four-chamber view acquisitions from 200 patients. The dataset will be split into three folds:\n",
    "\n",
    "- ***a training set*** (x_train,y_train) representing 50% of the full dataset. x_train contains a list of the input image will y_train contains a list of the corresponding reference masks \n",
    "- ***a validation set*** (x_val,y_val) representating 25% of the full dataset. This dataset will be used to select the network parameters which produce the best results on the training dataset.\n",
    "- ***a testing set*** (x_test, y_test) representing 25% of the full dataset. This dataset will be used to assess the scores that can reach the trained network on unseen data.\n",
    "\n",
    "**Instructions**: Let's get more familiar with the dataset. Load the different dataset by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "data_path = '/floyd/input/camus_dataset' # path to the dataset on floydhub\n",
    "train_ind = range(300, 400)\n",
    "val_ind = range(400, 450)\n",
    "test_ind = range(450, 500)\n",
    "\n",
    "# Call \"load_camus\" function to load data present on the data_path folder.\n",
    "# The second argument provides the list of indices to load a set of data\n",
    "# The third argument forces to resize each image with the given values\n",
    "# For more information on this function, go and explore it !\n",
    "(x_train, y_train) = load_camus(data_path, train_ind, size=(256, 256))\n",
    "(x_val, y_val) = load_camus(data_path, val_ind, size=(256, 256))\n",
    "(x_test, y_test) = load_camus(data_path, test_ind, size=(256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise**: The following lines allow you to see the size of the loaded training input images and references stored in the x_train and y_train variables. Execute it and modify it to also investigate the size of the validation and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of X: \", x_train.shape, \" || size of Y : \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: The last dimension of the y_train is 4. Indeed, in tensorflow/keras implementation, it is necessary to provide one binary channel per class to segment. Since we want to segment three different structures in this project (left ventricle, myocardium and left atrium), we set 4 channels, one for each structure plus one for the background. \n",
    "\n",
    "**Excercice**: Execute the following code to better assess this notion of channel. In particular select different channel and see the corresponding image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ind_channel = 2  # indice of channel\n",
    "img = y_train[0,:,:,ind_channel]\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: Let's visualize the dataset for different samples by executing the following cell. Feel free re-run the cell multiple times to see other images. For those interested with Python programming, don't hesitate to analyze the code of the *display_dataset_samples* function defined in the file *utils.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_examples = 4\n",
    "display_dataset_samples(nb_examples, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='architecture_unet'></a>\n",
    "## 3 - Definition of the architecture of a U-Net model\n",
    "    => estimated time: 40 minutes\n",
    "\n",
    "It is time to design our U-Net architecture from the Keras library ! This is done in our unet function coded in model.py. The prototype of this function is given below:\n",
    "\n",
    "***<div align=\"center\">def unet(input_size=(256, 256, 1), ori_nb_fm=16, pretrained_weights=None, use_bn=True, show_summary=False)):</div>***\n",
    "\n",
    "It takes several arguments as input:\n",
    "\n",
    "- ***input_size***: size of the input image provided to the network. By default this value is set to (256,256,1).\n",
    "- ***ori_nb_fm***: the original number of filter maps, that linearly evolves through the U-net. The feature maps are the output of a convolutional neural network. By default this value was set to 16 but it is often set at higher values to have larger networks with more parameters.\n",
    "- ***pretrained_weights***: provides parameters that have been already trained. By default the *None* value is set, meaning that the network has to be trained from scratch.\n",
    "- ***show_summary***: flag used for the display of the summary of the generated network (Keras function)  \n",
    "\n",
    "\n",
    "**Excercice**: Analyse the unet function given in the *model.py* file from the file explorer. In particular try to understand the overall definition of the network and draw a parallel with the U-Net diagram provided at the beginning of this hands-on session.\n",
    "\n",
    "\n",
    "**Comments**:\n",
    "- The functions *Convolution2D*, *BatchNormalization*, *Activation*, *MaxPooling2D* and *UpSampling2D* all belong to the Keras library.\n",
    "- Concerning the *Convolution2D* function, the different arguments correspond to: \n",
    "    - the number of features produced as output;\n",
    "    - the (x,y)-size of the convolution kernel;\n",
    "    - the padding used to deal with the intrinsic border effect of the convolution operation;\n",
    "    - the stride at which the convolution is applied;\n",
    "    - the type of initialization to perform on the underlying parameters.\n",
    "\n",
    "To better understand those arguments, we provide below a diagram which summarizes the calculations that are performed on these layers (this diagram was taken from the official U-Net website from the university of Freiburg).\n",
    "\n",
    "<img src=\"notebook_illustrations/convolution_layer.png\" style=\"width:868px;height:364px;\">\n",
    "\n",
    "\n",
    "**Instructions**: Execute the unet function and analyze the corresponding summary by running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = unet(input_size=(256, 256, 1), ori_nb_fm=16, use_bn=False, show_summary=True) # let's visualize the architecture to begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: From the diagram given above, can you justify the number of parameters corresponding to the first convolution layer that appear in the summary ? Same question but with the second convolution layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training_phase'></a>\n",
    "## 4 - Training phase\n",
    "    => estimated time: 40 minutes\n",
    "\n",
    "Now that you are familiar with the dataset and the definition of the unet architecture, it is time to learn the model from the training dataset. \n",
    "\n",
    "To this aim, we now have to focus our efforts on the following fundamental steps:\n",
    "- Definition of the optimization process\n",
    "- Assessment of the learning process\n",
    "- Evaluation of the derived model on the training and validation datasets\n",
    "\n",
    "\n",
    "### 4.1 - Definition of the optimization process\n",
    "\n",
    "There are several parameters that have to be defined to set up an optimization scheme in deep learning. Among the most important, we can mention:\n",
    "\n",
    "- ***batch size***: number of images used during one iteration (one forward/backward pass) of the optimization process\n",
    "- ***number of epochs***: number of times the optimization process goes through entire training set\n",
    "- ***learning rate***: dividing factor applied to the loss gradients to adapt the steps towards the optimum\n",
    "- ***optimization algorithm***: there exists different algorithms to perform the optimization process, the most used being the Adam's optimizer\n",
    "- ***loss function***: function that will be minimized during the optimization process\n",
    "- ***metric functions***: functions that will be computed during the optimization process to see there evolution. These functions usually provide information on the quality of the segmentation but are not used to update the network parameters.\n",
    "\n",
    "\n",
    "**Instructions**: Configure the optimization parameters by running the cell below. Feel free during the hands-on session to play with these parameters and study their influence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10      # How many images per optimization step\n",
    "nb_epochs = 50      # How many times do we feed in the entire training set\n",
    "learning_rate = 5e-4 # dividing factor applied to the loss gradients : take small step towards the optimum \n",
    "gradient_optimizer = Adam(learning_rate)   # weights updating algorithm\n",
    "loss_function = 'categorical_crossentropy' # function that will be minimized during the learning process\n",
    "our_metrics = [multiclass_dice]       # functions that provide information on the quality of the segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: To define the optimization parameters chosen for the deep learning model under Keras, it is necessary to use the *compile* function associated to the Keras model. Run the cell below to launch such a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet(input_size=(256, 256, 1), pretrained_weights = None)\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=gradient_optimizer,\n",
    "              metrics=our_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: Under Keras, we can set up callback functions that are linked to the model. These callback functions are useful to set important properties and additional functionalities during the learning process. In this hands-on session, we will use two different callback functions:\n",
    "\n",
    "- ***ModelCheckpoint***: this is a Keras function which allows the configuration of the instances where the current parameter values throughout the learning process are saved\n",
    "- ***LossHistory***: this is a callback function that we have coded in the *utils.py* file which allows the saving of the evolution of the loss and metric functions over the learning process. As you will see, the corresponding graph are extremely useful to assess the behavior of the training phase\n",
    "\n",
    "\n",
    "**Instructions**: We will use the two aformentioned callback functions for our project. In particular, we will use the ModelCheckPoint function to save the best parameters values throughout the training phase based on the loss obtained on the validation dataset. Run the cell below to define and link these two callback function to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best model will be saved under the unet.h5 file    \n",
    "model_name = 'unet.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'models')\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Callback to save the best model \n",
    "save_best = ModelCheckpoint(\n",
    "    model_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    period=1\n",
    "    )\n",
    "\n",
    "# Callback to keep track of losses and metrics\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**CONGRATULATIONS !** You have completed all the necessary steps to configure a deep learning model in Keras. As you can see, thanks to high-level libraries such as tensorflow/keras, this can be easily done !\n",
    "\n",
    "**Instructions**: This is time to launch your first learning process in order to fit the training dataset by running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train a model, use the fit Keras function attached to a model \n",
    "f = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epochs,\n",
    "              validation_data=(x_val, y_val),\n",
    "              shuffle=True,\n",
    "              verbose=2,\n",
    "              callbacks=[save_best, history],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: Have a look at the output values computed at each epoch. Analyze these values and interpret their variations throughout the learning process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Assessment of the learning process\n",
    "\n",
    "The most effective way to evaluate the learning process (*i.e.* to see how the training went) is to display and analyze the loss and metric curves stored using to the callback function *LossHistory* described above.\n",
    "\n",
    "**Instructions**: Display the loss and metric curves corresponding to the learning process you just executed by running the cell below. For those interested with Python programming, don't hesitate to analyze the code of the *display_loss_metric_curves* function defined in the file *utils.py*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig_dir = os.path.join(os.getcwd(), 'figures') # folder where to save the png file corresponding to the computed curves\n",
    "save_fig_filename = 'first_unet_train.png'   # filename of the png file saved in the folder defined above\n",
    "display_loss_metric_curves(nb_epochs, history, save_fig_dir, save_fig_filename) # our own function to display loss and metric curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: \n",
    "\n",
    "- What information can you retrieve from these curves ? \n",
    "- Why is it important to use a validation set ? \n",
    "- Why don't we use directly the testing dataset instead ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Evaluation of the trained U-Net on the training and validation datasets\n",
    "\n",
    "The loss and metric functions we used to assess the learning process are not sufficient to evaluate the performance of the learned network on the training and validation dataset. Usually, studies involving additional and complementary metrics are carried out.\n",
    "\n",
    "In segmentation domain, the standard metrics used to assess the geometrical quality corresponds to the Dice similarity index, Hausdorff distance (HD) and the average symmetric surface distance (ASSD) whose definitions are given below:\n",
    "\n",
    "- ***Dice similarity index***: defined as $D=2\\left(\\left|S_{user}\\cap S_{ref}\\right|\\right)/\\left(\\left|S_{user}\\right|+\\left|S_{ref}\\right|\\right)$. It is a measure of overlap between the segmented surface $S_{user}$ extracted from a method and the corresponding reference surface $S_{ref}$. The Dice index gives a value between 0 (no overlap) and 1 (full overlap)\n",
    "- ***Hausdorff distance (HD)***: measures the local maximum distance between two surfaces\n",
    "- ***Average Symmetric Surface Distance (ASSD)***: corresponds to the average distance between two surfaces\n",
    "\n",
    "In addition, segmentations obtained from a deep learning model can contain small isolated regions that are falsely detected. To overcome these issues, some post processing steps based on morphological operators or some connected component analysis are usually applied.\n",
    "\n",
    "**Instructions**: Run the cell below to calculate the metrics defined above on the training dataset and using the previously learned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants used for evaluation\n",
    "use_pp = True # Use post-processing on the prediction \n",
    "Metrics = ['Dice', 'HD (mm)', 'ASSD (mm)']\n",
    "Structs = ['LV', 'Myo', 'LA']\n",
    "\n",
    "# Run evaluation on the training set\n",
    "train_res, train_seg = get_geometrical_results(model, x_train, y_train, data_path, train_ind, use_pp)\n",
    "\n",
    "# Display average results\n",
    "display_mean_values_of_metrics(train_res, Metrics, Structs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: By taking into account that the resolution of an image acquired by a standard echocardiographic equipement is around 1 $mm^2$, analyze the accuracy of the obtained results.\n",
    "\n",
    "\n",
    "**Comments**: The average performance alone doesn't say much about the network's robustness. It is therefore preferable to plot the distributions of values for all metrics.\n",
    "\n",
    "**Instructions**: Run the cell below to compute and display the distribution of values for the three metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histograms \n",
    "display_histogram_of_metrics(train_res, Metrics, Structs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: It is now the time to look at the performance of our network on the validation dataset. Run the cell below to both compute the average score of your network on the validation dataset and display the corresponding histograms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on the validation set\n",
    "val_res, val_seg = get_geometrical_results(model, x_val, y_val, data_path, val_ind, use_pp)\n",
    "\n",
    "# Display average results\n",
    "display_mean_values_of_metrics(val_res, Metrics, Structs)\n",
    "\n",
    "# Display histograms \n",
    "display_histogram_of_metrics(val_res, Metrics, Structs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: Compare the overall performance of your U-Net model obtained on the training and the validation datasets. Do you see any differences ? If so, can you explain them ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Evaluation of the trained U-Net on the testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have trained your U-Net model and that you have observed that the learning process was carried out correctly, this means that your architecture should behave well on new data. This is why we have created the testing dataset. Indeed, your network has not seen any data of this set so far and we will test the network's ability to generalize to it. The final scores you will get for this dataset are the ones that you will need to keep to assess the performance of your network.\n",
    "\n",
    "\n",
    "**Instruction**: Run the cell below to both compute the average score of your network on the testing dataset and display the corresponding histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on the testing set\n",
    "test_res, test_seg = get_geometrical_results(model, x_test, y_test, data_path, test_ind, use_pp)\n",
    "\n",
    "# Display average results\n",
    "display_mean_values_of_metrics(test_res, Metrics, Structs)\n",
    "\n",
    "# Display histograms \n",
    "display_histogram_of_metrics(test_res, Metrics, Structs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: Compare the overall performance of your U-Net model obtained on the validation and the testing datasets. Can we say we picked a good validation set ? Where do the biases come from ?\n",
    "\n",
    "**Instructions**: To have a complete overview of the performance the learned network, assess visually the model predictions on samples from the testing dataset by running the cell below. Feel free re-run the cell multiple times to see other images. For those interested with Python programming, don't hesitate to analyze the code of the *display_dataset_samples_with_estimation* function defined in the file *utils.py*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize predictions on samples randomly selected from the testing dataset\n",
    "nb_examples = 4\n",
    "display_dataset_samples_with_estimation(nb_examples, x_test, y_test, test_seg, test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: Are you convinced visually by the obtained results ? What kind of additional tests are missing to be able to say that the model is ready for clinical applications ?\n",
    "\n",
    "<br>\n",
    "\n",
    "**Comments**: You can directly load a pre-trained model fom the unet function by passing as argument the filename of the corresponding weights that have been saved. For instance the cell below allows to load a pre-trained model from the file named *pretrained_unet_fh.h5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet(input_size=(256, 256, 1), pretrained_weights='/floyd/input/camus_pretrained_models/pretrained_unet_fh.h5')\n",
    "\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=gradient_optimizer,\n",
    "              metrics=our_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training_optimization'></a>\n",
    "## 5 - Training optimization \n",
    "    => estimated time: 30 minutes\n",
    "    \n",
    "In this part of the hands-on session, you will study the influence of important hyper-parameters involved in the learning process by playing with their values. Due to the limited time, we restrict this study to the following parameters:\n",
    "\n",
    "- ***learning rate***: dividing factor used during the optimization process and applied to the loss gradients to adapt the steps towards the optimum\n",
    "\n",
    "- ***batch size***: number of samples propagated through the network before each weight update\n",
    "\n",
    "**Exercice**: \n",
    "\n",
    "1. Run the cell below with the activation of the batch normalization and by playing with different values of the learning rate. We advise you to test learning rate values between 1e-1 and 1e-4 for 30 epochs. What happens ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# Fix parameters \n",
    "flag_batch_norm = True\n",
    "nb_initial_feature_map = 16\n",
    "batch_size = 10\n",
    "nb_epochs = 30\n",
    "gradient_optimizer = Adam(learning_rate)\n",
    "loss_function = 'categorical_crossentropy'\n",
    "\n",
    "# store details in a string\n",
    "details = ('unet' + '_lr_' + str(learning_rate) + '_loss_' + str(loss_function) +\n",
    "'_batch_size_' + str(batch_size) + '_epochs_' + str(nb_epochs) + '.png')\n",
    "\n",
    "# compile model \n",
    "model = unet(input_size=(256, 256, 1), pretrained_weights = None)\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=gradient_optimizer,\n",
    "              metrics=our_metrics)\n",
    "\n",
    "# prepare saving of the model\n",
    "model_name =  details + '.h5'\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "# callback functions\n",
    "save_best = ModelCheckpoint(\n",
    "    model_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    period=1\n",
    "    )\n",
    "history = LossHistory()\n",
    "\n",
    "# Train model\n",
    "f = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epochs,\n",
    "              validation_data=(x_val, y_val),\n",
    "              shuffle=True,\n",
    "              verbose=2,\n",
    "              callbacks=[save_best, history],\n",
    "        )\n",
    "\n",
    "# Save and display results\n",
    "save_fig_dir = os.path.join(os.getcwd(), 'figures') # folder where to save the png file corresponding to the computed curves\n",
    "save_fig_filename = details                         # filename of the png file saved in the folder defined above\n",
    "display_loss_metric_curves(nb_epochs, history, save_fig_dir, save_fig_filename) # our own function to display loss and metric curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='unet_optimization'></a>\n",
    "## 6 - U-Net architecture optimization \n",
    "    => estimated time: 30 minutes\n",
    "    \n",
    "In this part of the hands-on session, you will study the influence of important parameters involved in the U-Net architecture by playing with their values. Due to the limited time, we restrict this study to the following parameters:\n",
    "\n",
    "- ***number of initial feature maps***: feature maps correspond to the output of the convolution kernels applied at a specific layer. In the U-Net architecture we use, the number of feature maps are double when switching from one layer to another. Thus, the number of feature maps for all layers is fixed by the number of initial feature maps applied to the first layer. \n",
    "\n",
    "\n",
    "- ***batch normalization***: dividing weights that have to be learned so to normalize the dynamic of activation per convolutional kernels. This processing has a regularization effect on the network. \n",
    "\n",
    "**Exercices**: \n",
    "\n",
    "1. Run the cell below with the batch normalization activated, a learning rate of 1e-3, a number of epoch of 30 and by doubling the original number of feature maps (i.e. 16). What are the consequence on the model ? What can be observed on the training and validation losses ?\n",
    "\n",
    "\n",
    "2. Run the cell below with the original number of feature maps (i.e. 16), a learning rate of 1e-3, a number of epoch of 10 and by playing with the activation of the batch normalization. What are your observations on the training curves ? You can have a look to the following website to analyze these results: https://www.quora.com/Why-does-batch-normalization-help)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune\n",
    "flag_batch_norm = True\n",
    "nb_initial_feature_map = 16\n",
    "\n",
    "# Fix parameters \n",
    "learning_rate = 1e-3\n",
    "batch_size = 10\n",
    "nb_epochs = 30\n",
    "optimizer = Adam(learning_rate)\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "# store details in a string\n",
    "details = ('unet' + '_lr_' + str(learning_rate) + '_loss_' + str(loss) +\n",
    "'_batch_size_' + str(batch_size) + '_epochs_' + str(nb_epochs) )\n",
    "\n",
    "# compile model \n",
    "model = unet(input_size=(256, 256, 1), pretrained_weights = None)\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=gradient_optimizer,\n",
    "              metrics=our_metrics)\n",
    "\n",
    "# prepare saving of the model\n",
    "model_name =  details + '.h5'\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "# callback functions\n",
    "save_best = ModelCheckpoint(\n",
    "    model_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    period=1\n",
    "    )\n",
    "history = LossHistory()\n",
    "\n",
    "# Train model\n",
    "f = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epochs,\n",
    "              validation_data=(x_val, y_val),\n",
    "              shuffle=True,\n",
    "              verbose=2,\n",
    "              callbacks=[save_best, history],\n",
    "        )\n",
    "\n",
    "# Save and display results\n",
    "save_fig_dir = os.path.join(os.getcwd(), 'figures') # folder where to save the png file corresponding to the computed curves\n",
    "save_fig_filename = details                         # filename of the png file saved in the folder defined above\n",
    "display_loss_metric_curves(nb_epochs, history, save_fig_dir, save_fig_filename) # our own function to display loss and metric curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='to_go_further'></a>\n",
    "## 7 - To go further\n",
    "    => estimated time: remaining time :)\n",
    "\n",
    "\n",
    "### 7-1 : Influence of the training set size \n",
    "\n",
    "As you would expect, the size of the training dataset is critical to the quality of the final results. Surprisingly, this property is not studied in most of the article you will find in the literature! In this part of the hands-on session, you will investigate the impact of such a parameter by evaluating the scores achieved by the U-Net model trained from a varying size of the training dataset (10, 20, 50, 70, 80, 90, 100). To save you time, we already trained such models using the following hyper-parameters: activation of the batch normalization, learning rate of 1e-3, a number of epoch of 30, a number of initial feature maps of 16. These models are present in the pretrained_model dataset if you wish to use them.\n",
    "\n",
    "<img src=\"notebook_illustrations/inc_size.png\" style=\"width:868px;height:364px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: From the analysis of these curves, what conclusions can you draw about the size of the training dataset ? about the capacity of our network ?\n",
    "\n",
    "***\n",
    "\n",
    "### 7-2 : visualization of the feature maps\n",
    "\n",
    "After the learning of a deep learning model, it is great not to consider it as a black box, but to study what is happening within its architecture. In this part of the hands-on session, you will look at intermediary feature maps, especially the first and the last layers.\n",
    "\n",
    "**Instructions**: Run the cell below to visualize the first filters and feature maps that have been learned from a pre-trained network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get feature maps\n",
    "model = unet(input_size=(256, 256, 1), pretrained_weights='/floyd/input/camus_pretrained_models/pretrained_unet_fh.h5')\n",
    "layers = model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select a specific layer weights and feature maps\n",
    "feat_layer = layers[1] # extract the first layer\n",
    "weights = feat_layer.get_weights()[0] # extract the filters (not the biases at [1])\n",
    "interm_model = Model(inputs=model.input, outputs=feat_layer.output)\n",
    "\n",
    "# Apply the model on an image from the validation set\n",
    "ind = np.random.randint(0, x_val.shape[0], 1)\n",
    "feat_maps = interm_model.predict(x_val[ind].reshape(1, 256, 256, 1))\n",
    "feat_maps = np.squeeze(feat_maps, axis=0)\n",
    "\n",
    "# Display the size of the selected feature maps\n",
    "print('\\nSize of the selected feature maps', weights.shape)\n",
    "\n",
    "# display weights applied to the first feature map of the previous layer\n",
    "nb_feat = feat_maps.shape[-1]\n",
    "display_model_weights(weights, nb_feat)\n",
    "\n",
    "# Display the number of feature maps\n",
    "print('\\nNumber of feature maps', nb_feat)\n",
    "\n",
    "# Display the corresponding feature maps\n",
    "display_model_feature_maps(feat_maps, nb_feat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: \n",
    "\n",
    "- Try to interpret the action that performs the learned filters on the input image. Change the previous code to visualize the feature maps for the second layer.\n",
    "\n",
    "- To visualize the feature maps of the last layer, you simply have to change the third line of the previous code (feat_layer = layers[1]) by *feat_layer = layers[-1]*. Change the code accordingly and visualize the corresponding feature maps. What do these feature maps represent for the final segmentation task ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
